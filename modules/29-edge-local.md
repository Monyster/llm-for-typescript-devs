# –ú–æ–¥—É–ª—å 29: Edge AI —Ç–∞ local models ‚Äî Ollama, llama.cpp

## üéØ –©–æ –≤–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ –∑ —Ü—å–æ–≥–æ –º–æ–¥—É–ª—è

–ü—ñ—Å–ª—è –ø—Ä–æ—Ö–æ–¥–∂–µ–Ω–Ω—è –≤–∏ –±—É–¥–µ—Ç–µ:
- –ó–∞–ø—É—Å–∫–∞—Ç–∏ LLM –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ Ollama
- –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ local models –∑ AI SDK
- –†–æ–∑—É–º—ñ—Ç–∏ –∫–æ–ª–∏ self-hosted –º–æ–¥–µ–ª—å –∫—Ä–∞—â–µ –∑–∞ API
- –ó–Ω–∞—Ç–∏ –æ–±–º–µ–∂–µ–Ω–Ω—è —Ç–∞ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ open-source –º–æ–¥–µ–ª–µ–π

**–Ø–∫—ñ –∑–∞–¥–∞—á—ñ —Ü–µ –¥–æ–∑–≤–æ–ª—è—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏:** –ü–æ–≤–Ω–∞ –ø—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å (–¥–∞–Ω—ñ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç—å –≤–∞—à —Å–µ—Ä–≤–µ—Ä). –ù—É–ª—å–æ–≤–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É (–ø—ñ—Å–ª—è –ø–æ–∫—É–ø–∫–∏ GPU). –†–æ–±–æ—Ç–∞ offline. Compliance –≤–∏–º–æ–≥–∏ –¥–µ cloud API –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ.

---

## 29.1 –ö–æ–ª–∏ self-hosted, –∞ –∫–æ–ª–∏ API

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Cloud API | Self-hosted |
|----------|-----------|-------------|
| –í–∞—Ä—Ç—ñ—Å—Ç—å (–º–∞–ª–µ–Ω—å–∫–∏–π –æ–±—Å—è–≥) | $10-100/–º—ñ—Å | $0 (CPU) –∞–±–æ $500+ (GPU) |
| –í–∞—Ä—Ç—ñ—Å—Ç—å (–≤–µ–ª–∏–∫–∏–π –æ–±—Å—è–≥) | $1,000-10,000/–º—ñ—Å | –§—ñ–∫—Å–æ–≤–∞–Ω–∞ (hardware) |
| –ü—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å | –î–∞–Ω—ñ –π–¥—É—Ç—å –¥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ | –í—Å–µ –Ω–∞ –≤–∞—à–∏—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö |
| –Ø–∫—ñ—Å—Ç—å | Frontier models (GPT-5, Claude) | Open-source (–Ω–∏–∂—á–∞ —è–∫—ñ—Å—Ç—å) |
| –õ–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å | 200-2000ms (–º–µ—Ä–µ–∂–∞) | 50-500ms (–ª–æ–∫–∞–ª—å–Ω–æ) |
| –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è | –ú–∏—Ç—Ç—î–≤–µ | –ü–æ—Ç—Ä—ñ–±–µ–Ω hardware |
| Maintenance | –ù—É–ª—å | –í–∞—à–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å |

**Self-hosted –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –∫–æ–ª–∏:** –¥–∞–Ω—ñ –Ω–µ –º–æ–∂—É—Ç—å –ø–æ–∫–∏–¥–∞—Ç–∏ –º–µ—Ä–µ–∂—É (–º–µ–¥–∏—Ü–∏–Ω–∞, —Ñ—ñ–Ω–∞–Ω—Å–∏), –≤–µ–ª–∏–∫–∏–π –æ–±—Å—è–≥ –∑–∞–ø–∏—Ç—ñ–≤ (>100K/–¥–µ–Ω—å), –ø–æ—Ç—Ä—ñ–±–Ω–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å, –∞–±–æ offline –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è.

---

## 29.2 Ollama ‚Äî –Ω–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π —Å–ø–æ—Å—ñ–± –∑–∞–ø—É—Å—Ç–∏—Ç–∏ LLM –ª–æ–∫–∞–ª—å–Ω–æ

```bash
# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è (macOS, Linux, Windows)
curl -fsSL https://ollama.ai/install.sh | sh

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª—ñ
ollama pull llama3.2       # 3B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –ø—Ä–∞—Ü—é—î –Ω–∞ CPU
ollama pull qwen3:8b       # 8B, —Ö–æ—Ä–æ—à–∞ –¥–ª—è –∫–æ–¥—É
ollama pull deepseek-r1:7b # 7B, reasoning –º–æ–¥–µ–ª—å

# –ó–∞–ø—É—Å–∫ (API —Å–µ—Ä–≤–µ—Ä –Ω–∞ localhost:11434)
ollama serve
```

### –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ AI SDK

```typescript
import { generateText } from 'ai';
import { createOllama } from 'ollama-ai-provider';

const ollama = createOllama({
  baseURL: 'http://localhost:11434/api',
});

const { text } = await generateText({
  model: ollama('llama3.2'),
  prompt: '–ü–æ—è—Å–Ω–∏ —â–æ —Ç–∞–∫–µ TypeScript –æ–¥–Ω–∏–º —Ä–µ—á–µ–Ω–Ω—è–º.',
});

// –¢–æ–π –∂–µ –∫–æ–¥ —â–æ –π –∑ OpenAI ‚Äî –ø—Ä–æ—Å—Ç–æ —ñ–Ω—à–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä!
```

### –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è open-source –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ | RAM –ø–æ—Ç—Ä—ñ–±–Ω–æ | –Ø–∫—ñ—Å—Ç—å | –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è |
|--------|-----------|-------------|--------|-------------|
| Llama 3.2 3B | 3B | 4GB | –ë–∞–∑–æ–≤–∞ | –ü—Ä–æ—Å—Ç—ñ –∑–∞–¥–∞—á—ñ, CPU |
| Qwen3 8B | 8B | 6GB | –•–æ—Ä–æ—à–∞ | –ö–æ–¥, –∞–Ω–∞–ª—ñ–∑ |
| DeepSeek R1 7B | 7B | 6GB | –•–æ—Ä–æ—à–∞ | Reasoning |
| Llama 4 Scout | 109B (MoE) | 64GB+ | –í–∏—Å–æ–∫–∞ | GPU —Å–µ—Ä–≤–µ—Ä |
| Qwen3 235B | 235B (MoE) | 128GB+ | –î—É–∂–µ –≤–∏—Å–æ–∫–∞ | Production GPU |

---

## 29.3 –ì—ñ–±—Ä–∏–¥–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: Local + Cloud

–ù–∞–π–ø—Ä–∞–∫—Ç–∏—á–Ω—ñ—à–∏–π –ø—ñ–¥—Ö—ñ–¥ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ local models –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–∞–¥–∞—á —Ç–∞ cloud –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö:

```typescript
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createOllama } from 'ollama-ai-provider';

const ollama = createOllama();

async function smartRoute(prompt: string, complexity: 'simple' | 'complex') {
  if (complexity === 'simple') {
    // –õ–æ–∫–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å: –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ, –ø—Ä–∏–≤–∞—Ç–Ω–æ, —à–≤–∏–¥–∫–æ
    return generateText({ model: ollama('llama3.2'), prompt });
  } else {
    // Cloud: –Ω–∞–π–≤–∏—â–∞ —è–∫—ñ—Å—Ç—å –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á
    return generateText({ model: openai('gpt-4o-mini'), prompt });
  }
}

// PII-sensitive –¥–∞–Ω—ñ ‚Äî —Ç—ñ–ª—å–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ
async function processSensitiveData(data: string) {
  return generateText({
    model: ollama('qwen3:8b'),  // –î–∞–Ω—ñ –ù–ï –ø–æ–∫–∏–¥–∞—é—Ç—å —Å–µ—Ä–≤–µ—Ä
    prompt: `–í–∏—Ç—è–≥–Ω–∏ –º–µ–¥–∏—á–Ω—ñ –¥—ñ–∞–≥–Ω–æ–∑–∏ –∑ —Ç–µ–∫—Å—Ç—É: ${data}`,
  });
}
```

---

## 29.4 Deployment: GPU —Å–µ—Ä–≤–µ—Ä –¥–ª—è production

```bash
# Docker –∑ GPU –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é
docker run -d --gpus all \
  -v ollama_data:/root/.ollama \
  -p 11434:11434 \
  --name ollama \
  ollama/ollama

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
docker exec ollama ollama pull qwen3:8b
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –ø–æ hardware

| –ó–∞–¥–∞—á–∞ | GPU | RAM | –ú–æ–¥–µ–ª—å |
|--------|-----|-----|--------|
| –ü—Ä–æ—Ç–æ—Ç–∏–ø—É–≤–∞–Ω–Ω—è | –ë–µ–∑ GPU (CPU) | 8GB | Llama 3.2 3B |
| –ù–µ–≤–µ–ª–∏–∫–∏–π production | RTX 4090 (24GB) | 32GB | Qwen3 8B |
| –°–µ—Ä–µ–¥–Ω—ñ–π production | A100 (80GB) | 128GB | Llama 4 Scout |
| Enterprise | 4x A100 | 512GB | Qwen3 235B |

---

## –ü–µ—Ä–µ–≤—ñ—Ä —Å–µ–±–µ

1. –ö–æ–ª–∏ self-hosted –º–æ–¥–µ–ª—å –∫—Ä–∞—â–µ –∑–∞ cloud API?
2. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å Ollama —ñ –∑–∞–ø—É—Å—Ç—ñ—Ç—å Llama 3.2 –ª–æ–∫–∞–ª—å–Ω–æ
3. –ù–∞–ø–∏—à—ñ—Ç—å –∫–æ–¥ —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î local model —á–µ—Ä–µ–∑ AI SDK
4. –°–ø—Ä–æ–µ–∫—Ç—É–π—Ç–µ –≥—ñ–±—Ä–∏–¥–Ω—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É: local –¥–ª—è PII, cloud –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –∑–∞–¥–∞—á
5. –°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î GPU —Å–µ—Ä–≤–µ—Ä –Ω–∞ –º—ñ—Å—è—Ü—å –ø–æ—Ä—ñ–≤–Ω—è–Ω–æ –∑ API?

---

**–ù–∞–∑–∞–¥:** [‚Üê –ú–æ–¥—É–ª—å 28 ‚Äî Event-driven AI](28-event-driven.md) | **–î–∞–ª—ñ:** [–ú–æ–¥—É–ª—å 30 ‚Äî Real-world Use Cases ‚Üí](30-use-cases.md)
