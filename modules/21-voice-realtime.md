# –ú–æ–¥—É–ª—å 21: Voice —Ç–∞ Realtime API ‚Äî –ì–æ–ª–æ—Å–æ–≤—ñ –∞–≥–µ–Ω—Ç–∏

## üéØ –©–æ –≤–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ –∑ —Ü—å–æ–≥–æ –º–æ–¥—É–ª—è

–ü—ñ—Å–ª—è –ø—Ä–æ—Ö–æ–¥–∂–µ–Ω–Ω—è –≤–∏ –±—É–¥–µ—Ç–µ:
- –†–æ–∑—É–º—ñ—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –≥–æ–ª–æ—Å–æ–≤–∏—Ö AI-—Å–∏—Å—Ç–µ–º (STT ‚Üí LLM ‚Üí TTS)
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ OpenAI Realtime API –¥–ª—è –¥–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—å–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∑–≤'—è–∑–∫—É
- –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ Speech-to-Text —Ç–∞ Text-to-Speech —É —Å–≤–æ—ó –∑–∞—Å—Ç–æ—Å—É–Ω–∫–∏
- –ë—É–¥—É–≤–∞—Ç–∏ –≥–æ–ª–æ—Å–æ–≤–∏—Ö –∞–≥–µ–Ω—Ç—ñ–≤ –∑ function calling

**–Ø–∫—ñ –∑–∞–¥–∞—á—ñ —Ü–µ –¥–æ–∑–≤–æ–ª—è—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏:** –ì–æ–ª–æ—Å–æ–≤—ñ AI-–∞—Å–∏—Å—Ç–µ–Ω—Ç–∏ –¥–ª—è –∫–æ–ª-—Ü–µ–Ω—Ç—Ä—ñ–≤, voice-first —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∏ –¥–ª—è –º–æ–±—ñ–ª—å–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ñ —Å–∏—Å—Ç–µ–º–∏, –≥–æ–ª–æ—Å–æ–≤–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è IoT-–ø—Ä–∏—Å—Ç—Ä–æ—è–º–∏.

---

## 21.1 –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –≥–æ–ª–æ—Å–æ–≤–∏—Ö AI-—Å–∏—Å—Ç–µ–º

### –ö–ª–∞—Å–∏—á–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥: Pipeline

```
–ì–æ–ª–æ—Å ‚Üí [STT] ‚Üí –¢–µ–∫—Å—Ç ‚Üí [LLM] ‚Üí –¢–µ–∫—Å—Ç ‚Üí [TTS] ‚Üí –ì–æ–ª–æ—Å
         ‚Üë                                  ‚Üë
     Whisper/                          ElevenLabs/
     Deepgram                          OpenAI TTS
```

–¢—Ä–∏ –æ–∫—Ä–µ–º—ñ –∫—Ä–æ–∫–∏, –∫–æ–∂–µ–Ω –¥–æ–¥–∞—î –∑–∞—Ç—Ä–∏–º–∫—É (200-500ms –Ω–∞ –∫—Ä–æ–∫).

### –°—É—á–∞—Å–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥: OpenAI Realtime API

```
–ì–æ–ª–æ—Å ‚Üê‚Üí [Realtime API] ‚Üê‚Üí –ì–æ–ª–æ—Å
          (–≤—Å–µ –≤ –æ–¥–Ω–æ–º—É)
          –õ–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å: ~300ms
```

–û–¥–Ω–∞ WebSocket-—Å–µ—Å—ñ—è, –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É –æ–±—Ä–æ–±–ª—è—î —Ç–∞ –≥–µ–Ω–µ—Ä—É—î –∞—É–¥—ñ–æ.

---

## 21.2 Speech-to-Text: –ì–æ–ª–æ—Å ‚Üí –¢–µ–∫—Å—Ç

### OpenAI Whisper

```typescript
import OpenAI from 'openai';
import { createReadStream } from 'fs';

const client = new OpenAI();

// –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
const transcription = await client.audio.transcriptions.create({
  model: 'whisper-1',
  file: createReadStream('./meeting-recording.mp3'),
  language: 'uk',  // –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞
  response_format: 'verbose_json',  // –í–∫–ª—é—á–∞—î timestamps
});

console.log(transcription.text);
// "–ü—Ä–∏–≤—ñ—Ç, —Å—å–æ–≥–æ–¥–Ω—ñ –º–∏ –æ–±–≥–æ–≤–æ—Ä–∏–º–æ –ø–ª–∞–Ω –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –∫–≤–∞—Ä—Ç–∞–ª..."

// –ó timestamps
for (const segment of transcription.segments!) {
  console.log(`[${segment.start.toFixed(1)}s] ${segment.text}`);
}
```

### Deepgram (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞, realtime STT)

```typescript
// Deepgram ‚Äî streaming STT (—Ä–µ–∞–ª—å–Ω–∏–π —á–∞—Å)
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';

const deepgram = createClient(process.env.DEEPGRAM_API_KEY!);
const connection = deepgram.listen.live({
  model: 'nova-2',
  language: 'uk',
  smart_format: true,
  interim_results: true,
});

connection.on(LiveTranscriptionEvents.Transcript, (data) => {
  const transcript = data.channel.alternatives[0].transcript;
  if (transcript) {
    console.log(`[–ñ–∏–≤–∏–π —Ç–µ–∫—Å—Ç]: ${transcript}`);
  }
});

// –ù–∞–¥—Å–∏–ª–∞—î–º–æ –∞—É–¥—ñ–æ-–ø–æ—Ç—ñ–∫
microphone.on('data', (audioChunk: Buffer) => {
  connection.send(audioChunk);
});
```

---

## 21.3 Text-to-Speech: –¢–µ–∫—Å—Ç ‚Üí –ì–æ–ª–æ—Å

### OpenAI TTS

```typescript
import OpenAI from 'openai';
import { writeFileSync } from 'fs';

const client = new OpenAI();

const response = await client.audio.speech.create({
  model: 'tts-1-hd',     // 'tts-1' ‚Äî —à–≤–∏–¥—à–∏–π, 'tts-1-hd' ‚Äî —è–∫—ñ—Å–Ω—ñ—à–∏–π
  voice: 'nova',          // alloy, echo, fable, onyx, nova, shimmer
  input: '–ü—Ä–∏–≤—ñ—Ç! –Ø –≤–∞—à AI-–∞—Å–∏—Å—Ç–µ–Ω—Ç. –ß–∏–º –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏?',
  response_format: 'mp3',
  speed: 1.0,             // 0.25 - 4.0
});

// –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —É —Ñ–∞–π–ª
const buffer = Buffer.from(await response.arrayBuffer());
writeFileSync('response.mp3', buffer);
```

### ElevenLabs (–Ω–∞–π—è–∫—ñ—Å–Ω—ñ—à–∏–π –≥–æ–ª–æ—Å)

```typescript
// ElevenLabs ‚Äî –Ω–∞–π—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ—à—ñ –≥–æ–ª–æ—Å–∏, –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –∫–ª–æ–Ω—É–≤–∞–Ω–Ω—è
const response = await fetch(
  `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,
  {
    method: 'POST',
    headers: {
      'xi-api-key': process.env.ELEVENLABS_API_KEY!,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      text: '–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?',
      model_id: 'eleven_multilingual_v2',
      voice_settings: {
        stability: 0.5,
        similarity_boost: 0.75,
      },
    }),
  }
);

const audioBuffer = await response.arrayBuffer();
```

---

## 21.4 OpenAI Realtime API ‚Äî –≤—Å–µ –≤ –æ–¥–Ω–æ–º—É

Realtime API ‚Äî —Ü–µ WebSocket-–∑'—î–¥–Ω–∞–Ω–Ω—è –¥–µ –º–æ–¥–µ–ª—å –æ–¥–Ω–æ—á–∞—Å–Ω–æ —Å–ª—É—Ö–∞—î —Ç–∞ –≥–æ–≤–æ—Ä–∏—Ç—å:

```typescript
import OpenAI from 'openai';

const client = new OpenAI();

// –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Realtime —Å–µ—Å—ñ—ó
const session = await client.realtime.sessions.create({
  model: 'gpt-4o-realtime-preview',
  voice: 'nova',
  instructions: `–¢–∏ ‚Äî AI-–∞—Å–∏—Å—Ç–µ–Ω—Ç –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –¥–ª—è TechShop.
–ì–æ–≤–æ—Ä–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é. –ë—É–¥—å –¥—Ä—É–∂–Ω—ñ–º —Ç–∞ –ª–∞–∫–æ–Ω—ñ—á–Ω–∏–º.
–ú–æ–∂–µ—à –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è.`,
  tools: [
    {
      type: 'function',
      name: 'check_order',
      description: '–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–º–æ–≤–ª–µ–Ω–Ω—è',
      parameters: {
        type: 'object',
        properties: {
          order_id: { type: 'string' },
        },
        required: ['order_id'],
      },
    },
  ],
  turn_detection: {
    type: 'server_vad',  // –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–ª–∏ –ª—é–¥–∏–Ω–∞ –∑–∞–∫—ñ–Ω—á–∏–ª–∞ –≥–æ–≤–æ—Ä–∏—Ç–∏
    threshold: 0.5,
    prefix_padding_ms: 300,
    silence_duration_ms: 500,
  },
});

// WebSocket –∑'—î–¥–Ω–∞–Ω–Ω—è
const ws = new WebSocket(
  'wss://api.openai.com/v1/realtime',
  { headers: { 'Authorization': `Bearer ${process.env.OPENAI_API_KEY}` } }
);

ws.on('message', (data) => {
  const event = JSON.parse(data.toString());

  switch (event.type) {
    case 'response.audio.delta':
      // –ê—É–¥—ñ–æ-–≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –º–æ–¥–µ–ª—ñ ‚Äî –≤—ñ–¥—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–∞ –¥–∏–Ω–∞–º—ñ–∫—É
      playAudio(Buffer.from(event.delta, 'base64'));
      break;

    case 'response.function_call_arguments.done':
      // –ú–æ–¥–µ–ª—å —Ö–æ—á–µ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é
      const result = await handleToolCall(event.name, JSON.parse(event.arguments));
      // –í—ñ–¥–ø—Ä–∞–≤–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞–∑–∞–¥
      ws.send(JSON.stringify({
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: event.call_id,
          output: JSON.stringify(result),
        },
      }));
      break;

    case 'input_audio_buffer.speech_started':
      console.log('–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–æ—á–∞–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏...');
      break;

    case 'input_audio_buffer.speech_stopped':
      console.log('–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑–∞–∫—ñ–Ω—á–∏–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏');
      break;
  }
});

// –ù–∞–¥—Å–∏–ª–∞—î–º–æ –∞—É–¥—ñ–æ –∑ –º—ñ–∫—Ä–æ—Ñ–æ–Ω—É
microphone.on('data', (chunk: Buffer) => {
  ws.send(JSON.stringify({
    type: 'input_audio_buffer.append',
    audio: chunk.toString('base64'),
  }));
});
```

### –í–∞—Ä—Ç—ñ—Å—Ç—å Realtime API

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –¶—ñ–Ω–∞ |
|-----------|------|
| –ê—É–¥—ñ–æ –≤—Ö—ñ–¥ | $0.06 / —Ö–≤–∏–ª–∏–Ω–∞ |
| –ê—É–¥—ñ–æ –≤–∏—Ö—ñ–¥ | $0.24 / —Ö–≤–∏–ª–∏–Ω–∞ |
| –¢–µ–∫—Å—Ç–æ–≤–∏–π –≤—Ö—ñ–¥ | $5.00 / 1M —Ç–æ–∫–µ–Ω—ñ–≤ |
| –¢–µ–∫—Å—Ç–æ–≤–∏–π –≤–∏—Ö—ñ–¥ | $20.00 / 1M —Ç–æ–∫–µ–Ω—ñ–≤ |

–¢–∏–ø–æ–≤–∏–π 5-—Ö–≤–∏–ª–∏–Ω–Ω–∏–π –¥–∑–≤—ñ–Ω–æ–∫: ~$1.50 (–¥–µ—à–µ–≤—à–µ –∑–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–ª-—Ü–µ–Ω—Ç—Ä—É).

---

## 21.5 Pipeline –ø—ñ–¥—Ö—ñ–¥: STT + LLM + TTS

–î–ª—è –±—ñ–ª—å—à–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—é —Ç–∞ –Ω–∏–∂—á–æ—ó –≤–∞—Ä—Ç–æ—Å—Ç—ñ:

```typescript
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import OpenAI from 'openai';

const client = new OpenAI();

async function voiceAssistant(audioInput: Buffer): Promise<Buffer> {
  // 1. STT: –≥–æ–ª–æ—Å ‚Üí —Ç–µ–∫—Å—Ç
  const transcription = await client.audio.transcriptions.create({
    model: 'whisper-1',
    file: new File([audioInput], 'input.wav', { type: 'audio/wav' }),
    language: 'uk',
  });
  console.log(`[STT] ${transcription.text}`);

  // 2. LLM: —Ç–µ–∫—Å—Ç ‚Üí –≤—ñ–¥–ø–æ–≤—ñ–¥—å
  const { text } = await generateText({
    model: openai('gpt-4o-mini'),
    system: '–¢–∏ ‚Äî –≥–æ–ª–æ—Å–æ–≤–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ (1-2 —Ä–µ—á–µ–Ω–Ω—è).',
    prompt: transcription.text,
  });
  console.log(`[LLM] ${text}`);

  // 3. TTS: —Ç–µ–∫—Å—Ç ‚Üí –≥–æ–ª–æ—Å
  const speech = await client.audio.speech.create({
    model: 'tts-1',
    voice: 'nova',
    input: text,
    response_format: 'mp3',
  });
  console.log('[TTS] –ê—É–¥—ñ–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ');

  return Buffer.from(await speech.arrayBuffer());
}
```

---

## 21.6 –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—ñ–¥—Ö–æ–¥—ñ–≤

| –ö—Ä–∏—Ç–µ—Ä—ñ–π | Pipeline (STT+LLM+TTS) | OpenAI Realtime |
|----------|------------------------|-----------------|
| –õ–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å | 1-3 —Å–µ–∫—É–Ω–¥–∏ | ~300ms |
| –í–∞—Ä—Ç—ñ—Å—Ç—å | –ù–∏–∂—á–∞ (~$0.50/5—Ö–≤) | –í–∏—â–∞ (~$1.50/5—Ö–≤) |
| –ö–æ–Ω—Ç—Ä–æ–ª—å | –ü–æ–≤–Ω–∏–π (–∫–æ–∂–µ–Ω –∫—Ä–æ–∫ –æ–∫—Ä–µ–º–æ) | –û–±–º–µ–∂–µ–Ω–∏–π |
| –ü–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è | –°–∫–ª–∞–¥–Ω–æ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ | –í–±—É–¥–æ–≤–∞–Ω–µ |
| –ü—Ä–æ–≤–∞–π–¥–µ—Ä-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ñ—Å—Ç—å | –¢–∞–∫ (—Ä—ñ–∑–Ω—ñ STT/TTS) | –¢—ñ–ª—å–∫–∏ OpenAI |
| Function calling | –ß–µ—Ä–µ–∑ LLM | –í–±—É–¥–æ–≤–∞–Ω–µ |

---

## –ü–µ—Ä–µ–≤—ñ—Ä —Å–µ–±–µ

1. –ß–∏–º –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è Pipeline –ø—ñ–¥—Ö—ñ–¥ –≤—ñ–¥ Realtime API?
2. –ö–æ–ª–∏ Pipeline –∫—Ä–∞—â–µ –∑–∞ Realtime? –ö–æ–ª–∏ –Ω–∞–≤–ø–∞–∫–∏?
3. –°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î 10-—Ö–≤–∏–ª–∏–Ω–Ω–∏–π –≥–æ–ª–æ—Å–æ–≤–∏–π –¥–∑–≤—ñ–Ω–æ–∫ —á–µ—Ä–µ–∑ Realtime API?
4. –†–µ–∞–ª—ñ–∑—É–π—Ç–µ STT ‚Üí LLM ‚Üí TTS pipeline –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∞—Å–∏—Å—Ç–µ–Ω—Ç–∞
5. –Ø–∫ –¥–æ–¥–∞—Ç–∏ function calling –¥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞?

---

**–ù–∞–∑–∞–¥:** [‚Üê –ú–æ–¥—É–ª—å 20 ‚Äî Fine-tuning](20-fine-tuning.md) | **–î–∞–ª—ñ:** [–ú–æ–¥—É–ª—å 22 ‚Äî AI SDK UI –≥–ª–∏–±—à–µ ‚Üí](22-ai-sdk-ui.md)
